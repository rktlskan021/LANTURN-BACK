[{"text": "This is a 3.", "start": 4.22, "duration": 1.18}, {"text": "It's sloppily written and rendered at an extremely low resolution of 28x28 pixels, ", "start": 6.06, "duration": 4.709}, {"text": "but your brain has no trouble recognizing it as a 3.", "start": 10.769, "duration": 2.951}, {"text": "And I want you to take a moment to appreciate how ", "start": 14.34, "duration": 2.264}, {"text": "crazy it is that brains can do this so effortlessly.", "start": 16.604, "duration": 2.356}, {"text": "I mean, this, this and this are also recognizable as 3s, ", "start": 19.7, "duration": 3.319}, {"text": "even though the specific values of each pixel is very different from one ", "start": 23.019, "duration": 4.252}, {"text": "image to the next.", "start": 27.271, "duration": 1.049}, {"text": "The particular light-sensitive cells in your eye that are firing when ", "start": 28.9, "duration": 3.881}, {"text": "you see this 3 are very different from the ones firing when you see this 3.", "start": 32.781, "duration": 4.159}, {"text": "But something in that crazy-smart visual cortex of yours resolves these as representing ", "start": 37.52, "duration": 5.28}, {"text": "the same idea, while at the same time recognizing other images as their own distinct ", "start": 42.8, "duration": 5.1}, {"text": "ideas.", "start": 47.9, "duration": 0.36}, {"text": "But if I told you, hey, sit down and write for me a program that takes in a grid of ", "start": 49.22, "duration": 5.479}, {"text": "28x28 pixels like this and outputs a single number between 0 and 10, ", "start": 54.699, "duration": 4.501}, {"text": "telling you what it thinks the digit is, well the task goes from comically trivial to ", "start": 59.2, "duration": 5.61}, {"text": "dauntingly difficult.", "start": 64.81, "duration": 1.37}, {"text": "Unless you've been living under a rock, I think I hardly need to motivate the relevance ", "start": 67.16, "duration": 3.74}, {"text": "and importance of machine learning and neural networks to the present and to the future.", "start": 70.9, "duration": 3.74}, {"text": "But what I want to do here is show you what a neural network actually is, ", "start": 75.12, "duration": 3.882}, {"text": "assuming no background, and to help visualize what it's doing, ", "start": 79.002, "duration": 3.306}, {"text": "not as a buzzword but as a piece of math.", "start": 82.308, "duration": 2.152}, {"text": "My hope is that you come away feeling like the structure itself is motivated, ", "start": 85.02, "duration": 3.806}, {"text": "and to feel like you know what it means when you read, ", "start": 88.826, "duration": 2.683}, {"text": "or you hear about a neural network quote-unquote learning.", "start": 91.509, "duration": 2.831}, {"text": "This video is just going to be devoted to the structure component of that, ", "start": 95.36, "duration": 2.94}, {"text": "and the following one is going to tackle learning.", "start": 98.3, "duration": 1.96}, {"text": "What we're going to do is put together a neural ", "start": 100.96, "duration": 2.367}, {"text": "network that can learn to recognize handwritten digits.", "start": 103.327, "duration": 2.713}, {"text": "This is a somewhat classic example for introducing the topic, ", "start": 109.36, "duration": 2.744}, {"text": "and I'm happy to stick with the status quo here, ", "start": 112.104, "duration": 2.168}, {"text": "because at the end of the two videos I want to point you to a couple good ", "start": 114.272, "duration": 3.275}, {"text": "resources where you can learn more, and where you can download the code that ", "start": 117.547, "duration": 3.408}, {"text": "does this and play with it on your own computer.", "start": 120.955, "duration": 2.125}, {"text": "There are many many variants of neural networks, ", "start": 125.04, "duration": 2.675}, {"text": "and in recent years there's been sort of a boom in research towards these variants, ", "start": 127.715, "duration": 4.586}, {"text": "but in these two introductory videos you and I are just going to look at the simplest ", "start": 132.301, "duration": 4.695}, {"text": "plain vanilla form with no added frills.", "start": 136.996, "duration": 2.184}, {"text": "This is kind of a necessary prerequisite for understanding any of the more powerful ", "start": 139.86, "duration": 4.078}, {"text": "modern variants, and trust me it still has plenty of complexity for us to wrap our minds ", "start": 143.938, "duration": 4.322}, {"text": "around.", "start": 148.26, "duration": 0.34}, {"text": "But even in this simplest form it can learn to recognize handwritten digits, ", "start": 149.12, "duration": 4.128}, {"text": "which is a pretty cool thing for a computer to be able to do.", "start": 153.248, "duration": 3.272}, {"text": "And at the same time you'll see how it does fall ", "start": 157.48, "duration": 2.375}, {"text": "short of a couple hopes that we might have for it.", "start": 159.855, "duration": 2.425}, {"text": "As the name suggests neural networks are inspired by the brain, but let's break that down.", "start": 163.38, "duration": 5.12}, {"text": "What are the neurons, and in what sense are they linked together?", "start": 168.52, "duration": 3.14}, {"text": "Right now when I say neuron all I want you to think about is a thing that holds a number, ", "start": 172.5, "duration": 5.582}, {"text": "specifically a number between 0 and 1.", "start": 178.082, "duration": 2.358}, {"text": "It's really not more than that.", "start": 180.68, "duration": 1.88}, {"text": "For example the network starts with a bunch of neurons corresponding to ", "start": 183.78, "duration": 5.113}, {"text": "each of the 28x28 pixels of the input image, which is 784 neurons in total.", "start": 188.893, "duration": 5.327}, {"text": "Each one of these holds a number that represents the grayscale value of the ", "start": 194.7, "duration": 4.777}, {"text": "corresponding pixel, ranging from 0 for black pixels up to 1 for white pixels.", "start": 199.477, "duration": 4.903}, {"text": "This number inside the neuron is called its activation, ", "start": 205.3, "duration": 3.007}, {"text": "and the image you might have in mind here is that each neuron is lit up when its ", "start": 208.307, "duration": 4.349}, {"text": "activation is a high number.", "start": 212.656, "duration": 1.504}, {"text": "So all of these 784 neurons make up the first layer of our network.", "start": 216.72, "duration": 5.14}, {"text": "Now jumping over to the last layer, this has 10 neurons, ", "start": 226.5, "duration": 2.978}, {"text": "each representing one of the digits.", "start": 229.478, "duration": 1.882}, {"text": "The activation in these neurons, again some number that's between 0 and 1, ", "start": 232.04, "duration": 4.638}, {"text": "represents how much the system thinks that a given image corresponds with a given digit.", "start": 236.678, "duration": 5.442}, {"text": "There's also a couple layers in between called the hidden layers, ", "start": 243.04, "duration": 3.433}, {"text": "which for the time being should just be a giant question mark for ", "start": 246.473, "duration": 3.433}, {"text": "how on earth this process of recognizing digits is going to be handled.", "start": 249.906, "duration": 3.694}, {"text": "In this network I chose two hidden layers, each one with 16 neurons, ", "start": 254.26, "duration": 3.652}, {"text": "and admittedly that's kind of an arbitrary choice.", "start": 257.912, "duration": 2.648}, {"text": "To be honest I chose two layers based on how I want to motivate the structure ", "start": 261.019, "duration": 3.545}, {"text": "in just a moment, and 16, well that was just a nice number to fit on the screen.", "start": 264.564, "duration": 3.636}, {"text": "In practice there is a lot of room for experiment with a specific structure here.", "start": 268.78, "duration": 3.56}, {"text": "The way the network operates, activations in one ", "start": 273.02, "duration": 2.702}, {"text": "layer determine the activations of the next layer.", "start": 275.722, "duration": 2.758}, {"text": "And of course the heart of the network as an information processing mechanism comes down ", "start": 279.2, "duration": 4.663}, {"text": "to exactly how those activations from one layer bring about activations in the next layer.", "start": 283.863, "duration": 4.717}, {"text": "It's meant to be loosely analogous to how in biological networks of neurons, ", "start": 289.14, "duration": 4.552}, {"text": "some groups of neurons firing cause certain others to fire.", "start": 293.692, "duration": 3.488}, {"text": "Now the network I'm showing here has already been trained to recognize digits, ", "start": 298.12, "duration": 3.505}, {"text": "and let me show you what I mean by that.", "start": 301.625, "duration": 1.775}, {"text": "It means if you feed in an image, lighting up all 784 neurons of the input layer ", "start": 303.64, "duration": 4.711}, {"text": "according to the brightness of each pixel in the image, ", "start": 308.351, "duration": 3.258}, {"text": "that pattern of activations causes some very specific pattern in the next layer ", "start": 311.609, "duration": 4.653}, {"text": "which causes some pattern in the one after it, ", "start": 316.262, "duration": 2.734}, {"text": "which finally gives some pattern in the output layer.", "start": 318.996, "duration": 3.084}, {"text": "And the brightest neuron of that output layer is the network's choice, ", "start": 322.56, "duration": 4.013}, {"text": "so to speak, for what digit this image represents.", "start": 326.573, "duration": 2.827}, {"text": "And before jumping into the math for how one layer influences the next, ", "start": 332.56, "duration": 3.83}, {"text": "or how training works, let's just talk about why it's even reasonable ", "start": 336.39, "duration": 3.724}, {"text": "to expect a layered structure like this to behave intelligently.", "start": 340.114, "duration": 3.406}, {"text": "What are we expecting here?", "start": 344.06, "duration": 1.16}, {"text": "What is the best hope for what those middle layers might be doing?", "start": 345.4, "duration": 2.2}, {"text": "Well, when you or I recognize digits, we piece together various components.", "start": 348.92, "duration": 4.6}, {"text": "A 9 has a loop up top and a line on the right.", "start": 354.2, "duration": 2.62}, {"text": "An 8 also has a loop up top, but it's paired with another loop down low.", "start": 357.38, "duration": 3.8}, {"text": "A 4 basically breaks down into three specific lines, and things like that.", "start": 361.98, "duration": 4.84}, {"text": "Now in a perfect world, we might hope that each neuron in the second ", "start": 367.6, "duration": 4.015}, {"text": "to last layer corresponds with one of these subcomponents, ", "start": 371.615, "duration": 3.434}, {"text": "that anytime you feed in an image with, say, a loop up top, ", "start": 375.049, "duration": 3.492}, {"text": "like a 9 or an 8, there's some specific neuron whose activation is going to be close to 1.", "start": 378.541, "duration": 5.239}, {"text": "And I don't mean this specific loop of pixels, ", "start": 384.5, "duration": 2.457}, {"text": "the hope would be that any generally loopy pattern towards the top sets off this neuron.", "start": 386.957, "duration": 4.603}, {"text": "That way, going from the third layer to the last one just requires ", "start": 392.44, "duration": 3.663}, {"text": "learning which combination of subcomponents corresponds to which digits.", "start": 396.103, "duration": 3.937}, {"text": "Of course, that just kicks the problem down the road, ", "start": 401.0, "duration": 2.241}, {"text": "because how would you recognize these subcomponents, ", "start": 403.241, "duration": 2.199}, {"text": "or even learn what the right subcomponents should be?", "start": 405.44, "duration": 2.2}, {"text": "And I still haven't even talked about how one layer influences the next, ", "start": 408.06, "duration": 3.201}, {"text": "but run with me on this one for a moment.", "start": 411.261, "duration": 1.799}, {"text": "Recognizing a loop can also break down into subproblems.", "start": 413.68, "duration": 3.0}, {"text": "One reasonable way to do this would be to first ", "start": 417.28, "duration": 2.666}, {"text": "recognize the various little edges that make it up.", "start": 419.946, "duration": 2.834}, {"text": "Similarly, a long line, like the kind you might see in the digits 1 or 4 or 7, ", "start": 423.78, "duration": 4.677}, {"text": "is really just a long edge, or maybe you think of it as a certain pattern of several ", "start": 428.457, "duration": 5.034}, {"text": "smaller edges.", "start": 433.491, "duration": 0.829}, {"text": "So maybe our hope is that each neuron in the second layer of ", "start": 435.14, "duration": 3.728}, {"text": "the network corresponds with the various relevant little edges.", "start": 438.868, "duration": 3.852}, {"text": "Maybe when an image like this one comes in, it lights up all of the ", "start": 443.54, "duration": 4.03}, {"text": "neurons associated with around 8 to 10 specific little edges, ", "start": 447.57, "duration": 3.674}, {"text": "which in turn lights up the neurons associated with the upper loop ", "start": 451.244, "duration": 3.971}, {"text": "and a long vertical line, and those light up the neuron associated with a 9.", "start": 455.215, "duration": 4.505}, {"text": "Whether or not this is what our final network actually does is another question, ", "start": 460.68, "duration": 4.053}, {"text": "one that I'll come back to once we see how to train the network, ", "start": 464.733, "duration": 3.253}, {"text": "but this is a hope that we might have, a sort of goal with the layered structure ", "start": 467.986, "duration": 4.053}, {"text": "like this.", "start": 472.039, "duration": 0.501}, {"text": "Moreover, you can imagine how being able to detect edges and patterns ", "start": 473.16, "duration": 3.648}, {"text": "like this would be really useful for other image recognition tasks.", "start": 476.808, "duration": 3.492}, {"text": "And even beyond image recognition, there are all sorts of intelligent ", "start": 480.88, "duration": 3.177}, {"text": "things you might want to do that break down into layers of abstraction.", "start": 484.057, "duration": 3.223}, {"text": "Parsing speech, for example, involves taking raw audio and picking out distinct sounds, ", "start": 488.04, "duration": 4.743}, {"text": "which combine to make certain syllables, which combine to form words, ", "start": 492.783, "duration": 3.773}, {"text": "which combine to make up phrases and more abstract thoughts, etc.", "start": 496.556, "duration": 3.504}, {"text": "But getting back to how any of this actually works, ", "start": 501.1, "duration": 2.635}, {"text": "picture yourself right now designing how exactly the activations in one layer might ", "start": 503.735, "duration": 4.258}, {"text": "determine the activations in the next.", "start": 507.993, "duration": 1.927}, {"text": "The goal is to have some mechanism that could conceivably combine pixels into edges, ", "start": 510.86, "duration": 5.189}, {"text": "or edges into patterns, or patterns into digits.", "start": 516.049, "duration": 2.931}, {"text": "And to zoom in on one very specific example, let's say the ", "start": 519.44, "duration": 3.584}, {"text": "hope is for one particular neuron in the second layer to pick ", "start": 523.024, "duration": 3.768}, {"text": "up on whether or not the image has an edge in this region here.", "start": 526.792, "duration": 3.828}, {"text": "The question at hand is what parameters should the network have?", "start": 531.44, "duration": 3.66}, {"text": "What dials and knobs should you be able to tweak so that it's expressive ", "start": 535.64, "duration": 4.065}, {"text": "enough to potentially capture this pattern, or any other pixel pattern, ", "start": 539.705, "duration": 4.009}, {"text": "or the pattern that several edges can make a loop, and other such things?", "start": 543.714, "duration": 4.066}, {"text": "Well, what we'll do is assign a weight to each one of the ", "start": 548.72, "duration": 3.148}, {"text": "connections between our neuron and the neurons from the first layer.", "start": 551.868, "duration": 3.692}, {"text": "These weights are just numbers.", "start": 556.32, "duration": 1.38}, {"text": "Then take all of those activations from the first layer ", "start": 558.54, "duration": 3.418}, {"text": "and compute their weighted sum according to these weights.", "start": 561.958, "duration": 3.542}, {"text": "I find it helpful to think of these weights as being organized into a ", "start": 567.7, "duration": 3.446}, {"text": "little grid of their own, and I'm going to use green pixels to indicate positive weights, ", "start": 571.146, "duration": 4.43}, {"text": "and red pixels to indicate negative weights, where the brightness of ", "start": 575.576, "duration": 3.397}, {"text": "that pixel is some loose depiction of the weight's value.", "start": 578.973, "duration": 2.807}, {"text": "Now if we made the weights associated with almost all of the pixels zero ", "start": 582.78, "duration": 3.799}, {"text": "except for some positive weights in this region that we care about, ", "start": 586.579, "duration": 3.538}, {"text": "then taking the weighted sum of all the pixel values really just amounts ", "start": 590.117, "duration": 3.799}, {"text": "to adding up the values of the pixel just in the region that we care about.", "start": 593.916, "duration": 3.904}, {"text": "And if you really wanted to pick up on whether there's an edge here, ", "start": 599.14, "duration": 3.299}, {"text": "what you might do is have some negative weights associated with the surrounding pixels.", "start": 602.439, "duration": 4.161}, {"text": "Then the sum is largest when those middle pixels ", "start": 607.48, "duration": 2.61}, {"text": "are bright but the surrounding pixels are darker.", "start": 610.09, "duration": 2.61}, {"text": "When you compute a weighted sum like this, you might come out with any number, ", "start": 614.26, "duration": 4.443}, {"text": "but for this network what we want is for activations to be some value between 0 and 1.", "start": 618.703, "duration": 4.837}, {"text": "So a common thing to do is to pump this weighted sum into some function ", "start": 624.12, "duration": 4.184}, {"text": "that squishes the real number line into the range between 0 and 1.", "start": 628.304, "duration": 3.836}, {"text": "And a common function that does this is called the sigmoid function, ", "start": 632.46, "duration": 3.422}, {"text": "also known as a logistic curve.", "start": 635.882, "duration": 1.538}, {"text": "Basically very negative inputs end up close to 0, ", "start": 638.0, "duration": 3.185}, {"text": "positive inputs end up close to 1, and it just steadily increases around the input 0.", "start": 641.185, "duration": 5.415}, {"text": "So the activation of the neuron here is basically a ", "start": 649.12, "duration": 3.585}, {"text": "measure of how positive the relevant weighted sum is.", "start": 652.705, "duration": 3.655}, {"text": "But maybe it's not that you want the neuron to ", "start": 657.54, "duration": 2.147}, {"text": "light up when the weighted sum is bigger than 0.", "start": 659.687, "duration": 2.193}, {"text": "Maybe you only want it to be active when the sum is bigger than say 10.", "start": 662.28, "duration": 4.08}, {"text": "That is, you want some bias for it to be inactive.", "start": 666.84, "duration": 3.42}, {"text": "What we'll do then is just add in some other number like negative 10 to this ", "start": 671.38, "duration": 4.14}, {"text": "weighted sum before plugging it through the sigmoid squishification function.", "start": 675.52, "duration": 4.14}, {"text": "That additional number is called the bias.", "start": 680.58, "duration": 1.86}, {"text": "So the weights tell you what pixel pattern this neuron in the second ", "start": 683.46, "duration": 3.906}, {"text": "layer is picking up on, and the bias tells you how high the weighted ", "start": 687.366, "duration": 3.907}, {"text": "sum needs to be before the neuron starts getting meaningfully active.", "start": 691.273, "duration": 3.907}, {"text": "And that is just one neuron.", "start": 696.12, "duration": 1.56}, {"text": "Every other neuron in this layer is going to be connected to ", "start": 698.28, "duration": 4.266}, {"text": "all 784 pixel neurons from the first layer, and each one of ", "start": 702.546, "duration": 4.197}, {"text": "those 784 connections has its own weight associated with it.", "start": 706.743, "duration": 4.197}, {"text": "Also, each one has some bias, some other number that you add ", "start": 711.6, "duration": 3.024}, {"text": "on to the weighted sum before squishing it with the sigmoid.", "start": 714.624, "duration": 2.976}, {"text": "And that's a lot to think about!", "start": 718.11, "duration": 1.43}, {"text": "With this hidden layer of 16 neurons, that's a total of 784 times 16 weights, ", "start": 719.96, "duration": 6.318}, {"text": "along with 16 biases.", "start": 726.278, "duration": 1.702}, {"text": "And all of that is just the connections from the first layer to the second.", "start": 728.84, "duration": 3.1}, {"text": "The connections between the other layers also have ", "start": 732.52, "duration": 2.41}, {"text": "a bunch of weights and biases associated with them.", "start": 734.93, "duration": 2.41}, {"text": "All said and done, this network has almost exactly 13,000 total weights and biases.", "start": 738.34, "duration": 5.46}, {"text": "13,000 knobs and dials that can be tweaked and ", "start": 743.8, "duration": 2.895}, {"text": "turned to make this network behave in different ways.", "start": 746.695, "duration": 3.265}, {"text": "So when we talk about learning, what that's referring to is ", "start": 751.04, "duration": 3.276}, {"text": "getting the computer to find a valid setting for all of these ", "start": 754.316, "duration": 3.385}, {"text": "many many numbers so that it'll actually solve the problem at hand.", "start": 757.701, "duration": 3.659}, {"text": "One thought experiment that is at once fun and kind of horrifying is to imagine sitting ", "start": 762.62, "duration": 4.618}, {"text": "down and setting all of these weights and biases by hand, ", "start": 767.238, "duration": 3.044}, {"text": "purposefully tweaking the numbers so that the second layer picks up on edges, ", "start": 770.282, "duration": 4.093}, {"text": "the third layer picks up on patterns, etc.", "start": 774.375, "duration": 2.205}, {"text": "I personally find this satisfying rather than just treating the network as a total ", "start": 776.98, "duration": 4.114}, {"text": "black box, because when the network doesn't perform the way you anticipate, ", "start": 781.094, "duration": 3.767}, {"text": "if you've built up a little bit of a relationship with what those weights and biases ", "start": 784.861, "duration": 4.213}, {"text": "actually mean, you have a starting place for experimenting with how to change the ", "start": 789.074, "duration": 4.065}, {"text": "structure to improve.", "start": 793.139, "duration": 1.041}, {"text": "Or when the network does work but not for the reasons you might expect, ", "start": 794.96, "duration": 3.522}, {"text": "digging into what the weights and biases are doing is a good way to challenge ", "start": 798.482, "duration": 3.815}, {"text": "your assumptions and really expose the full space of possible solutions.", "start": 802.297, "duration": 3.523}, {"text": "By the way, the actual function here is a little cumbersome to write down, ", "start": 806.84, "duration": 3.164}, {"text": "don't you think?", "start": 810.004, "duration": 0.676}, {"text": "So let me show you a more notationally compact way that these connections are represented.", "start": 812.5, "duration": 4.64}, {"text": "This is how you'd see it if you choose to read up more about neural networks.\n214\n00:13:41,380 --> 00:13:40,520\nOrganize all of the activations from one layer into a column as a vector.", "start": 817.66, "duration": 2.86}, {"text": "Then organize all of the weights as a matrix, where each row of that matrix corresponds ", "start": 821.38, "duration": 8.757}, {"text": "to the connections between one layer and a particular neuron in the next layer.", "start": 830.137, "duration": 7.863}, {"text": "What that means is that taking the weighted sum of the activations in ", "start": 838.54, "duration": 3.726}, {"text": "the first layer according to these weights corresponds to one of the ", "start": 842.266, "duration": 3.674}, {"text": "terms in the matrix vector product of everything we have on the left here.", "start": 845.94, "duration": 3.94}, {"text": "By the way, so much of machine learning just comes down to having a ", "start": 854.0, "duration": 3.508}, {"text": "good grasp of linear algebra, so for any of you who want a nice visual ", "start": 857.508, "duration": 3.663}, {"text": "understanding for matrices and what matrix vector multiplication means, ", "start": 861.171, "duration": 3.714}, {"text": "take a look at the series I did on linear algebra, especially chapter 3.", "start": 864.885, "duration": 3.715}, {"text": "Back to our expression, instead of talking about adding the bias to each one of ", "start": 869.24, "duration": 4.408}, {"text": "these values independently, we represent it by organizing all those biases into a vector, ", "start": 873.648, "duration": 4.959}, {"text": "and adding the entire vector to the previous matrix vector product.", "start": 878.607, "duration": 3.693}, {"text": "Then as a final step, I'll wrap a sigmoid around the outside here, ", "start": 883.28, "duration": 3.587}, {"text": "and what that's supposed to represent is that you're going to apply the ", "start": 886.867, "duration": 3.856}, {"text": "sigmoid function to each specific component of the resulting vector inside.", "start": 890.723, "duration": 4.017}, {"text": "So once you write down this weight matrix and these vectors as their own symbols, ", "start": 895.94, "duration": 4.593}, {"text": "you can communicate the full transition of activations from one layer to the next in an ", "start": 900.533, "duration": 4.93}, {"text": "extremely tight and neat little expression, and this makes the relevant code both a lot ", "start": 905.463, "duration": 4.93}, {"text": "simpler and a lot faster, since many libraries optimize the heck out of matrix ", "start": 910.393, "duration": 4.426}, {"text": "multiplication.", "start": 914.819, "duration": 0.841}, {"text": "Remember how earlier I said these neurons are simply things that hold numbers?", "start": 917.82, "duration": 3.64}, {"text": "Well of course the specific numbers that they hold depends on the image you feed in, ", "start": 922.22, "duration": 5.17}, {"text": "so it's actually more accurate to think of each neuron as a function, ", "start": 927.39, "duration": 4.258}, {"text": "one that takes in the outputs of all the neurons in the previous layer and spits out a ", "start": 931.648, "duration": 5.292}, {"text": "number between 0 and 1.", "start": 936.94, "duration": 1.4}, {"text": "Really the entire network is just a function, one that takes in ", "start": 939.2, "duration": 3.992}, {"text": "784 numbers as an input and spits out 10 numbers as an output.", "start": 943.192, "duration": 3.868}, {"text": "It's an absurdly complicated function, one that involves 13,000 parameters ", "start": 947.56, "duration": 3.954}, {"text": "in the forms of these weights and biases that pick up on certain patterns, ", "start": 951.514, "duration": 3.955}, {"text": "and which involves iterating many matrix vector products and the sigmoid ", "start": 955.469, "duration": 3.849}, {"text": "squishification function, but it's just a function nonetheless.", "start": 959.318, "duration": 3.322}, {"text": "And in a way it's kind of reassuring that it looks complicated.", "start": 963.4, "duration": 3.26}, {"text": "I mean if it were any simpler, what hope would we have ", "start": 967.34, "duration": 2.404}, {"text": "that it could take on the challenge of recognizing digits?", "start": 969.744, "duration": 2.536}, {"text": "And how does it take on that challenge?", "start": 973.34, "duration": 1.36}, {"text": "How does this network learn the appropriate weights and biases just by looking at data?", "start": 975.08, "duration": 4.28}, {"text": "Well that's what I'll show in the next video, and I'll also dig a little ", "start": 980.14, "duration": 3.096}, {"text": "more into what this particular network we're seeing is really doing.", "start": 983.236, "duration": 2.884}, {"text": "Now is the point I suppose I should say subscribe to stay notified ", "start": 987.58, "duration": 3.216}, {"text": "about when that video or any new videos come out, ", "start": 990.796, "duration": 2.399}, {"text": "but realistically most of you don't actually receive notifications from YouTube, do you?", "start": 993.195, "duration": 4.225}, {"text": "Maybe more honestly I should say subscribe so that the neural networks ", "start": 998.02, "duration": 3.302}, {"text": "that underlie YouTube's recommendation algorithm are primed to believe ", "start": 1001.322, "duration": 3.302}, {"text": "that you want to see content from this channel get recommended to you.", "start": 1004.624, "duration": 3.256}, {"text": "Anyway, stay posted for more.", "start": 1008.56, "duration": 1.38}, {"text": "Thank you very much to everyone supporting these videos on Patreon.", "start": 1010.76, "duration": 2.74}, {"text": "I've been a little slow to progress in the probability series this summer, ", "start": 1014.0, "duration": 3.485}, {"text": "but I'm jumping back into it after this project, ", "start": 1017.485, "duration": 2.277}, {"text": "so patrons you can look out for updates there.", "start": 1019.762, "duration": 2.138}, {"text": "To close things off here I have with me Lisha Li who did her PhD work on the ", "start": 1023.6, "duration": 3.535}, {"text": "theoretical side of deep learning and who currently works at a venture capital ", "start": 1027.135, "duration": 3.627}, {"text": "firm called Amplify Partners who kindly provided some of the funding for this video.", "start": 1030.762, "duration": 3.857}, {"text": "So Lisha one thing I think we should quickly bring up is this sigmoid function.", "start": 1035.46, "duration": 3.659}, {"text": "As I understand it early networks use this to squish the relevant weighted ", "start": 1039.7, "duration": 3.504}, {"text": "sum into that interval between zero and one, you know kind of motivated ", "start": 1043.204, "duration": 3.365}, {"text": "by this biological analogy of neurons either being inactive or active.", "start": 1046.569, "duration": 3.271}, {"text": "Exactly.", "start": 1050.28, "duration": 0.02}, {"text": "But relatively few modern networks actually use sigmoid anymore.", "start": 1050.56, "duration": 3.48}, {"text": "Yeah.", "start": 1054.32, "duration": 0.0}, {"text": "It's kind of old school right?", "start": 1054.44, "duration": 1.1}, {"text": "Yeah or rather ReLU seems to be much easier to train.", "start": 1055.76, "duration": 3.22}, {"text": "And ReLU, ReLU stands for rectified linear unit?", "start": 1059.4, "duration": 2.94}, {"text": "Yes it's this kind of function where you're just taking a max of zero ", "start": 1062.68, "duration": 4.789}, {"text": "and a where a is given by what you were explaining in the video and ", "start": 1067.469, "duration": 4.653}, {"text": "what this was sort of motivated from I think was a partially by a ", "start": 1072.122, "duration": 4.516}, {"text": "biological analogy with how neurons would either be activated or not.", "start": 1076.638, "duration": 4.722}, {"text": "And so if it passes a certain threshold it would be the identity function but if it did ", "start": 1081.36, "duration": 4.713}, {"text": "not then it would just not be activated so it'd be zero so it's kind of a simplification.", "start": 1086.073, "duration": 4.767}, {"text": "Using sigmoids didn't help training or it was very difficult ", "start": 1091.16, "duration": 4.39}, {"text": "to train at some point and people just tried ReLU and it happened ", "start": 1095.55, "duration": 4.751}, {"text": "to work very well for these incredibly deep neural networks.", "start": 1100.301, "duration": 4.319}, {"text": "All right thank you Lisha.", "start": 1105.1, "duration": 0.54}]