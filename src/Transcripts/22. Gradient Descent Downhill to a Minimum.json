[{"text": "The following content is\nprovided under a Creative Commons license.", "start": 1.55, "duration": 3.76}, {"text": "Your support will help\nMIT Open Courseware continue to offer high quality\neducational resources for free.", "start": 5.31, "duration": 6.3}, {"text": "To make a donation or to\nview additional materials from hundreds of\nMIT courses, visit MITopencourseware@ocw.MIT.edu.", "start": 11.61, "duration": 6.930000000000001}, {"text": "GILBERT STRANG: So I'm going to\ntalk about the gradient descent today to get to that\ncentral algorithm of neural net deep\nlearning, machine learning, and optimization in general.", "start": 24.17, "duration": 16.36}, {"text": "So I'm trying to\nminimize a function.", "start": 40.53, "duration": 2.7}, {"text": "And that's the way you do it if\nthere are many, many variables, too many to take\nsecond derivatives, then we settle for first\nderivatives of the function.", "start": 43.23, "duration": 13.65}, {"text": "So I introduced,\nand you've already met the idea of gradient.", "start": 56.88, "duration": 4.73}, {"text": "But let me just be sure\nto make some comments about the gradient\nand the Hessian and the role of convexity before\nwe see the big crucial example.", "start": 61.61, "duration": 14.0}, {"text": "So I've kind of prepared over\nhere for this crucial example.", "start": 75.61, "duration": 3.815}, {"text": "The function is a pure\nquadratic, two unknowns, x and y, pure quadratic.", "start": 82.01, "duration": 8.23}, {"text": "So every pure quadratic\nI can write in terms of a symmetric matrix s.", "start": 90.24, "duration": 6.92}, {"text": "And in this case, x1 squared\nwas bx2 squared, the symmetric, the matrix is just 2 by 2.", "start": 97.16, "duration": 8.65}, {"text": "It's diagonal.", "start": 105.81, "duration": 1.23}, {"text": "It's got eigenvalues 1 and\nb sitting on the diagonal.", "start": 107.04, "duration": 5.4}, {"text": "I'm thinking of b as\nbeing the smaller one.", "start": 112.44, "duration": 3.58}, {"text": "So the condition\nnumber, which we'll see, is all important in the question\nof the speed of convergence is the ratio of the\nlargest to the smallest.", "start": 116.02, "duration": 17.240000000000002}, {"text": "In this case, the largest\nis 1 the smallest is b.", "start": 133.26, "duration": 4.05}, {"text": "So that's 1 over b.", "start": 137.31, "duration": 1.95}, {"text": "And when 1 over b\nis a big number, when b is a very small\nnumber, then that's when we're in trouble.", "start": 139.26, "duration": 7.83}, {"text": "When the matrix is symmetric,\nthat condition number is lambda max over lambda min.", "start": 151.56, "duration": 6.0600000000000005}, {"text": "If I had an\nunsymmetric matrix, I would probably use sigma max\nover sigma min, of course.", "start": 157.62, "duration": 6.74}, {"text": "But here, matrices\nare symmetric.", "start": 164.36, "duration": 4.3}, {"text": "We're going to\nsee something neat is that we can actually take\nthe steps of steepest descent, write down what\neach step gives us, and see how quickly they\nconverge to the answer.", "start": 168.66, "duration": 16.65}, {"text": "And what is the answer?", "start": 185.31, "duration": 1.91}, {"text": "So I haven't put in\nany linear term here.", "start": 187.22, "duration": 4.15}, {"text": "So I just have a bowl\nsitting on the origin.", "start": 191.37, "duration": 3.36}, {"text": "So of course, the minimum\npoint is x equal 0, y equals 0.", "start": 194.73, "duration": 4.26}, {"text": "So the minimum point x\nstar, is 0, 0, of course.", "start": 198.99, "duration": 7.06}, {"text": "So the question will be how\nquickly do we get to that one.", "start": 206.05, "duration": 3.62}, {"text": "And you will say pretty\nsmall example, not typical.", "start": 209.67, "duration": 3.78}, {"text": "But the terrific\nthing is that we see everything for this example.", "start": 213.45, "duration": 5.4399999999999995}, {"text": "We can see the actual\nsteps of steepest descent.", "start": 218.89, "duration": 4.49}, {"text": "We can see how\nquickly they converge to the x star, the\nanswer, the place where this thing is a minimum.", "start": 223.38, "duration": 9.51}, {"text": "And we can begin to think\nwhat to do if it's too slow.", "start": 232.89, "duration": 8.55}, {"text": "So I'll come to that example\nafter some general thoughts about gradients, Hessians.", "start": 241.44, "duration": 8.4}, {"text": "So what does the\ngradient tell us?", "start": 249.84, "duration": 2.46}, {"text": "So let me just take an\nexample of the gradient.", "start": 252.3, "duration": 2.445}, {"text": "Let me take a linear function,\nf of xy equals say, 2x plus 5y.", "start": 257.86, "duration": 6.12}, {"text": "I just think we ought to get\ntotally familiar with these.", "start": 266.56, "duration": 4.98}, {"text": "We're doing something.", "start": 271.54, "duration": 2.37}, {"text": "We're jumping into\nan important topic.", "start": 273.91, "duration": 4.89}, {"text": "When I ask you\nwhat's the gradient, that's a freshman question.", "start": 278.8, "duration": 4.98}, {"text": "But let's just be sure we know\nhow to interpret the gradient, how to compute\nit, what it means, how to see it geometrically.", "start": 283.78, "duration": 10.42}, {"text": "So what's the gradient\nof that function?", "start": 294.2, "duration": 2.45}, {"text": "It's a function\nof two variables.", "start": 296.65, "duration": 1.73}, {"text": "So the gradient is a\nvector with two components.", "start": 298.38, "duration": 3.73}, {"text": "And they are?", "start": 302.11, "duration": 0.87}, {"text": "The derivative of\nthis factor x, which is 2 and the derivative of\nthis factor y, which is 5.", "start": 307.54, "duration": 5.779999999999999}, {"text": "So in this case, the\ngradient is constant.", "start": 313.32, "duration": 3.78}, {"text": "And the Hessian, which I\noften call H after Hessian, or del squared F\nwould tell us we're taking the second\nderivatives, that will be the second derivatives\nobviously 0 in this case.", "start": 317.1, "duration": 16.049999999999997}, {"text": "So what shape is H here?", "start": 333.15, "duration": 5.08}, {"text": "It's 2 by 2.", "start": 338.23, "duration": 1.5}, {"text": "Everybody recognizes 2 by\n2 is H would have the-- I'll take a second\nderivative of that-- sorry, the first derivative\nof that with respect to x, obviously 0, the first\nderivative with respect to y, the first derivative\nof that with respect to x y.", "start": 339.73, "duration": 20.89}, {"text": "Anyway, Hessian 0 for sure.", "start": 360.62, "duration": 4.22}, {"text": "So let me draw the surface.", "start": 364.84, "duration": 3.24}, {"text": "So x, y, and the surface, if\nI graph F in this direction, then obviously, I have a plane.", "start": 368.08, "duration": 8.879999999999999}, {"text": "And I'm at a typical point\non the plane let's say.", "start": 376.96, "duration": 3.88}, {"text": "Yeah, yeah.", "start": 380.84, "duration": 1.07}, {"text": "So I'm at a point\nx, y, I should say.", "start": 381.91, "duration": 2.16}, {"text": "I'm at a point x, y.", "start": 384.07, "duration": 1.62}, {"text": "And let me put the\nplane through it.", "start": 385.69, "duration": 2.65}, {"text": "So how do I interpret\nthe gradient at that particular point x, y?", "start": 388.34, "duration": 3.8950000000000005}, {"text": "What does 2x plus 5y tell me?", "start": 395.63, "duration": 2.61}, {"text": "Or rather what does grad\nF tell me about movement from that point x, y?", "start": 398.24, "duration": 12.27}, {"text": "Of course, the\ngradient is constant.", "start": 410.51, "duration": 1.52}, {"text": "So it really didn't matter\nwhat point I'm moving from.", "start": 412.03, "duration": 3.1}, {"text": "But taking a point here.", "start": 415.13, "duration": 2.55}, {"text": "So what's the deal if I move?", "start": 417.68, "duration": 2.61}, {"text": "What's the fastest way\nto go up the surface?", "start": 420.29, "duration": 3.72}, {"text": "If I took the plane that\nwent through that point x, y, what's the fastest way\nto climb the plane?", "start": 424.01, "duration": 7.609999999999999}, {"text": "What direction goes up fastest?", "start": 431.62, "duration": 3.01}, {"text": "The gradient direction, right?", "start": 434.63, "duration": 1.6}, {"text": "The gradient direction\nis the way up.", "start": 436.23, "duration": 2.85}, {"text": "How am I going to put\nit in this picture?", "start": 439.08, "duration": 3.62}, {"text": "I guess I'm thinking\nof this plane as-- so what plane?", "start": 442.7, "duration": 4.83}, {"text": "You could well ask what\nplane have I drawn?", "start": 447.53, "duration": 2.7}, {"text": "Suppose I've drawn the plane\n2x plus 5y equals 0 even?", "start": 450.23, "duration": 9.12}, {"text": "So I'll make it go\nthrough the arc.", "start": 459.35, "duration": 2.21}, {"text": "And I've taken a typical\npoint on that plane.", "start": 461.56, "duration": 2.98}, {"text": "Now if I want to\nincrease that function, I go perpendicular to the plane.", "start": 464.54, "duration": 8.16}, {"text": "If I want to stay level\nwith the function, if I wanted to stay at\n0, I stay in the plane.", "start": 472.7, "duration": 5.92}, {"text": "So there are two key directions.", "start": 478.62, "duration": 2.03}, {"text": "Everybody knows this.", "start": 480.65, "duration": 1.23}, {"text": "I'm just repeating.", "start": 481.88, "duration": 1.32}, {"text": "This is the direction\nof the gradient of F out of the plane, steepest upwards.", "start": 483.2, "duration": 7.050000000000001}, {"text": "This is the downwards\ndirection minus gradient of F, perpendicular to\nthe plane downwards.", "start": 490.25, "duration": 6.6899999999999995}, {"text": "And that line is in the plane.", "start": 496.94, "duration": 4.86}, {"text": "That's part of the level set.", "start": 501.8, "duration": 1.86}, {"text": "2x plus 5y equals 0\nwould be a level set.", "start": 503.66, "duration": 4.41}, {"text": "That's my pretty\namateur picture.", "start": 508.07, "duration": 4.88}, {"text": "Just all I want to remember is\nthese words level and steepest, up or down.", "start": 512.95, "duration": 16.38}, {"text": "Down with a minus sign that\nwe see in steepest descent.", "start": 529.33, "duration": 5.28}, {"text": "So where in steepest descent.", "start": 534.61, "duration": 4.37}, {"text": "And what's the Hessian\ntelling me about the surface if I take the matrix\nof second derivatives?", "start": 543.02, "duration": 9.79}, {"text": "So I have this surface.", "start": 552.81, "duration": 1.87}, {"text": "So I have a surface\nF equal constant.", "start": 554.68, "duration": 3.39}, {"text": "That's the sort\nof level surface.", "start": 562.99, "duration": 2.63}, {"text": "So if I stay in that surface,\nthe gradient of F is 0.", "start": 565.62, "duration": 3.91}, {"text": "Gradient of F is 0 in-- on-- on is a better word-- on the surface.", "start": 569.53, "duration": 6.761}, {"text": "The gradient of F\npoints perpendicular.", "start": 583.33, "duration": 2.89}, {"text": "But what about the Hessian,\nthe second derivative?", "start": 586.22, "duration": 11.88}, {"text": "What is that telling\nme about that surface in particular when the Hessian\nis 0 or other surfaces?", "start": 598.1, "duration": 9.85}, {"text": "What does the Hessian\ntell me about-- I'm thinking of the Hessian\nat a particular point.", "start": 607.95, "duration": 6.0649999999999995}, {"text": "So I'm getting 0 for the Hessian\nbecause the surface is flat.", "start": 616.99, "duration": 8.59}, {"text": "If the surface was\nconvex upwards from-- if it was a convex or a graph\nof F, the Hessian would be-- so I just want to make\nthat connection now.", "start": 625.58, "duration": 18.665}, {"text": "What's the connection between\nthe Hessian and convexity of the-- the Hessian of the function\nand convexity of the function?", "start": 648.81, "duration": 11.85}, {"text": "So the point is that convexity-- the Hessian tells me whether\nor not the surface is convex.", "start": 660.66, "duration": 9.69}, {"text": "And what is the test?", "start": 670.35, "duration": 1.2}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 671.55, "duration": 1.05}, {"text": "GILBERT STRANG: Positive\ndefinite or semi definite.", "start": 672.6, "duration": 3.75}, {"text": "I'm just looking for\nan excuse to write down convexity and strong.", "start": 676.35, "duration": 10.56}, {"text": "Do I say strict or\nstrong convexity?", "start": 686.91, "duration": 2.85}, {"text": "I've forgotten.", "start": 689.76, "duration": 0.87}, {"text": "Strict, I think.", "start": 690.63, "duration": 1.52}, {"text": "Strictly convex.", "start": 692.15, "duration": 0.88}, {"text": "So convexity, the Hessian\nis positive semi-definite, or which includes-- I better say that right here-- includes positive definite.", "start": 698.23, "duration": 13.844}, {"text": "If I'm looking for\na strict convexity, then I must require\npositive definite.", "start": 718.38, "duration": 4.84}, {"text": "H is positive definite.", "start": 723.22, "duration": 2.643}, {"text": "Semi-definite won't do.", "start": 729.81, "duration": 2.49}, {"text": "So semi-definite for convex.", "start": 732.3, "duration": 3.0}, {"text": "So that in fact,\nthe linear function is convex, but not\nstrictly convex.", "start": 735.3, "duration": 6.87}, {"text": "Strictly means it\nreally bends upwards.", "start": 742.17, "duration": 2.99}, {"text": "The Hessian is\npositive definite.", "start": 745.16, "duration": 1.73}, {"text": "The curvatures are positive.", "start": 746.89, "duration": 4.23}, {"text": "So this would include\nlinear functions, and that would not\ninclude linear function.", "start": 751.12, "duration": 6.34}, {"text": "They're not strictly convex.", "start": 757.46, "duration": 3.28}, {"text": "Good, good, good.", "start": 760.74, "duration": 1.77}, {"text": "Some examples-- OK, the\nnumber one example, of course, is the one we're\ntalking about over here.", "start": 762.51, "duration": 6.9}, {"text": "So examples f of x equal\n1/2 x transpose Sx.", "start": 769.41, "duration": 10.43}, {"text": "And of course, I could\nhave linear terms minus a transpose\nx, a linear term.", "start": 783.02, "duration": 7.290000000000001}, {"text": "And I could have a constant.", "start": 790.31, "duration": 2.46}, {"text": "OK.", "start": 792.77, "duration": 0.5}, {"text": "So this function\nis strictly convex when S is positive\ndefinite, because H is now S for that function,\nfor that function H. Usually H, the Hessian is\nvarying from point to point.", "start": 798.79, "duration": 20.38}, {"text": "The nice thing about a pure\nquadratic is its constant.", "start": 819.17, "duration": 3.6}, {"text": "It's the same S at all points.", "start": 822.77, "duration": 3.78}, {"text": "Let me just ask you-- so that's a convex function.", "start": 826.55, "duration": 6.82}, {"text": "And what's its minimum?", "start": 833.37, "duration": 2.88}, {"text": "What's the gradient,\nfirst of all?", "start": 836.25, "duration": 1.633}, {"text": "What's the gradient of that?", "start": 837.883, "duration": 1.167}, {"text": "I'm asking really\nfor differentiating thinking in vector, doing all\nn derivatives at once here.", "start": 843.79, "duration": 10.65}, {"text": "I'm asking for the whole\nvector of first derivatives.", "start": 854.44, "duration": 5.4}, {"text": "Because here I'm giving\nyou the whole function with x for vector x.", "start": 859.84, "duration": 8.31}, {"text": "Of course, we could\ntake n to be 1.", "start": 868.15, "duration": 3.06}, {"text": "And then we would\nsee that if n was 1, this would just be Sx\nsquared, half Sx squared.", "start": 871.21, "duration": 8.67}, {"text": "And the derivative of\na half Sx squared-- let me just put that\nover here so we're sure to get it right--\nhalf of Sx squared.", "start": 879.88, "duration": 8.82}, {"text": "This is in the n equal 1 case.", "start": 888.7, "duration": 2.79}, {"text": "And the derivative\nis obviously Sx.", "start": 891.49, "duration": 2.37}, {"text": "And that's what it is here, Sx.", "start": 893.86, "duration": 1.68}, {"text": "It's obviously\nsimple, but if you haven't thought\nabout that line, it's asking for all the\nfirst derivatives of that quadratic function.", "start": 906.49, "duration": 14.360000000000001}, {"text": "Oh!", "start": 920.85, "duration": 0.72}, {"text": "It's not-- What do I\nhave to include now here?", "start": 921.57, "duration": 6.37}, {"text": "That's not right as it stands\nfor the function that's written above it.", "start": 927.94, "duration": 4.577}, {"text": "What's the right gradient?", "start": 932.517, "duration": 1.083}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 933.6, "duration": 0.917}, {"text": "GILBERT STRANG: Minus a, thanks.", "start": 934.517, "duration": 3.703}, {"text": "Because the linear function,\nits partial derivatives are obviously just\nthe components of a.", "start": 938.22, "duration": 6.9}, {"text": "And the Hessian H is S,\nderivatives of that guy.", "start": 945.12, "duration": 10.91}, {"text": "OK.", "start": 956.03, "duration": 0.67}, {"text": "Good.", "start": 956.7, "duration": 0.6}, {"text": "Good, good, good.", "start": 957.3, "duration": 2.25}, {"text": "And the minimum value-- we\nmight as well-- oh yeah!", "start": 959.55, "duration": 2.97}, {"text": "What's the right words\nfor a minimum value?", "start": 962.52, "duration": 5.3}, {"text": "No, I'm sorry.", "start": 967.82, "duration": 1.75}, {"text": "The right word is\nminimum value like f min.", "start": 969.57, "duration": 4.86}, {"text": "So I want to compute f min.", "start": 974.43, "duration": 3.45}, {"text": "Well, first I have to figure out\nwhere is that minimum reached?", "start": 977.88, "duration": 6.05}, {"text": "And what's the answer to that?", "start": 983.93, "duration": 3.21}, {"text": "We're putting everything on\nthe board for this simple case.", "start": 987.14, "duration": 3.7}, {"text": "The minimum of f\nof f of f of x-- remember, it's x is--\nwe're in n dimensions-- is at x equal what?", "start": 990.84, "duration": 19.07}, {"text": "Well, the minimum is\nwhere the gradient is 0.", "start": 1009.91, "duration": 2.49}, {"text": "So what's the minimizing x?", "start": 1015.46, "duration": 3.921}, {"text": "S inverse a, thanks.", "start": 1019.381, "duration": 1.734}, {"text": "Sorry.", "start": 1028.18, "duration": 1.08}, {"text": "That's not right.", "start": 1029.26, "duration": 3.22}, {"text": "It's here that I\nmeant to write it.", "start": 1032.48, "duration": 1.54}, {"text": "Really, my whole point\nfor this little moment is to be sure that\nwe keep straight what I mean by the place where\nthe minimum is reached and the minimum value.", "start": 1037.099, "duration": 12.061}, {"text": "Those are two different things.", "start": 1049.16, "duration": 1.44}, {"text": "So the minimum is\nreached at S inverse a, because that's obviously\nwhere the gradient is 0.", "start": 1054.33, "duration": 5.9399999999999995}, {"text": "It's the solution to Sx equal a.", "start": 1060.27, "duration": 2.803}, {"text": "And what I was going to ask\nyou is what's the right word-- well, sort of word, made up\nword-- for this point x star where the minimum is reached?", "start": 1063.073, "duration": 15.687000000000001}, {"text": "So it's not the minimum value.", "start": 1078.76, "duration": 1.4}, {"text": "It's the point\nwhere it's reached.", "start": 1080.16, "duration": 1.56}, {"text": "And that's called-- the\nnotation for that point is AUDIENCE: Arg min.", "start": 1081.72, "duration": 5.271}, {"text": "GILBERT STRANG: Arg min, thanks.", "start": 1086.991, "duration": 3.249}, {"text": "Arg min of my function.", "start": 1090.24, "duration": 6.38}, {"text": "And that means the place-- the point where f equals f min.", "start": 1096.62, "duration": 8.298}, {"text": "I haven't said yet what\nthe minimum value is.", "start": 1108.2, "duration": 2.4}, {"text": "This tells us the point.", "start": 1110.6, "duration": 1.23}, {"text": "And that's usually what\nwe're interested in.", "start": 1111.83, "duration": 2.46}, {"text": "We're, to tell the\ntruth, not that interested in a typical example\nand what the minimum value is as much as where is it?", "start": 1114.29, "duration": 9.45}, {"text": "Where do we reach that thing?", "start": 1123.74, "duration": 2.85}, {"text": "And of course, so this is x min.", "start": 1126.59, "duration": 3.9}, {"text": "This is then arg min\nof my function f.", "start": 1130.49, "duration": 9.52}, {"text": "That's the point.", "start": 1140.01, "duration": 0.93}, {"text": "And it happens to\nbe in this case, the minimum value is actually 0.", "start": 1140.94, "duration": 5.58}, {"text": "Because there's no linear\nterm a transpose x.", "start": 1151.47, "duration": 3.72}, {"text": "Why am I talking about arg\nmin when you've all seen it?", "start": 1160.08, "duration": 6.19}, {"text": "I guess I think that\nsomebody could just be reading this stuff,\nfor example, learning about neural net, and run\ninto this expression arg min and think what's that?", "start": 1166.27, "duration": 17.09}, {"text": "So it's maybe a right\ntime to say what it is.", "start": 1183.36, "duration": 4.26}, {"text": "It's the point where\nthe minimum is reached.", "start": 1187.62, "duration": 2.49}, {"text": "Why those words, by the way?", "start": 1192.93, "duration": 2.58}, {"text": "Well, arg isn't much of a word.", "start": 1195.51, "duration": 1.77}, {"text": "It sounds like you're\ngetting strangled.", "start": 1197.28, "duration": 2.88}, {"text": "But it's sort of short.", "start": 1200.16, "duration": 3.36}, {"text": "I assume it's short.", "start": 1203.52, "duration": 1.92}, {"text": "Nobody ever told me this.", "start": 1205.44, "duration": 1.86}, {"text": "I assume it's\nshort for argument.", "start": 1207.3, "duration": 2.91}, {"text": "The word argument is a kind of\nlong word for the value of x.", "start": 1210.21, "duration": 4.95}, {"text": "If I have a function\nf of x, f, I call it function and x is the\nargument of that function.", "start": 1215.16, "duration": 8.61}, {"text": "You might more often\nsee the word variable.", "start": 1223.77, "duration": 3.66}, {"text": "But argument-- and I'm assuming\nthat's what that refers to, it's the argument that\nminimizes the function.", "start": 1227.43, "duration": 8.0}, {"text": "OK, good.", "start": 1235.43, "duration": 1.75}, {"text": "And here it is, S inverse a.", "start": 1237.18, "duration": 3.91}, {"text": "Now but just by the\nway, what is f min?", "start": 1241.09, "duration": 2.09}, {"text": "Do you know the\nminimum of a quadratic?", "start": 1243.18, "duration": 2.55}, {"text": "I mean, this is the fundamental\nminimization question, to minimize a quadratic.", "start": 1245.73, "duration": 6.93}, {"text": "Electrical engineering, a\nquadratic regulator problem is the simplest problem there.", "start": 1252.66, "duration": 5.62}, {"text": "There could be constraints.", "start": 1258.28, "duration": 1.64}, {"text": "And we'll see it with\nconstraints included.", "start": 1259.92, "duration": 3.15}, {"text": "But right now, no\nconstraints at all.", "start": 1263.07, "duration": 3.19}, {"text": "We're just looking at\nthe function f of x.", "start": 1266.26, "duration": 2.3}, {"text": "Let me to remove the\nb, because that just shifts the function by b.", "start": 1271.48, "duration": 6.65}, {"text": "If I erase that, just\nto say it didn't matter.", "start": 1278.13, "duration": 4.58}, {"text": "It's really that function.", "start": 1282.71, "duration": 2.29}, {"text": "So that function\nactually goes through 0.", "start": 1285.0, "duration": 3.03}, {"text": "As it is, when x is\n0, we obviously get 0.", "start": 1288.03, "duration": 4.26}, {"text": "But it's still on its\nway down, so to speak.", "start": 1292.29, "duration": 3.66}, {"text": "It's on its way down to\nthis point, S inverse a.", "start": 1295.95, "duration": 4.14}, {"text": "That's where it bottoms out.", "start": 1300.09, "duration": 2.4}, {"text": "And when it bottoms out,\nwhat do you get for f?", "start": 1302.49, "duration": 4.57}, {"text": "One thing I know, it's\ngoing to be negative because it passed through 0,\nand it was on its way below 0.", "start": 1307.06, "duration": 6.5600000000000005}, {"text": "So let's just figure\nout what that f min is.", "start": 1313.62, "duration": 3.6}, {"text": "So I have a half.", "start": 1317.22, "duration": 2.79}, {"text": "I'm just going to plug in S\ninverse a, the bottom point into the function, and see\nwhere the surface bottoms out and at what level\nit bottoms out.", "start": 1320.01, "duration": 15.69}, {"text": "So I have a half.", "start": 1335.7, "duration": 1.5}, {"text": "So that's S inverse a is\na transpose S inverse.", "start": 1337.2, "duration": 6.12}, {"text": "S symmetric, so I'll just\nwrite this inverse transpose.", "start": 1343.32, "duration": 3.63}, {"text": "S, S inverse a from\nthe quadratic term, minus a transpose.", "start": 1346.95, "duration": 10.82}, {"text": "And x is S inverse a.", "start": 1357.77, "duration": 2.26}, {"text": "Have you done this calculation?", "start": 1360.03, "duration": 2.55}, {"text": "It just doesn't\nhurt to repeat it.", "start": 1362.58, "duration": 3.66}, {"text": "So I've plugged in S inverse\na there, there, and there.", "start": 1366.24, "duration": 7.29}, {"text": "OK, what have I got?", "start": 1373.53, "duration": 1.53}, {"text": "Well, S inverse\ncancels S. So I have a half of a transpose\nS inverse a minus 1 of a transpose inverse a.", "start": 1375.06, "duration": 9.09}, {"text": "So I get finally\nnegative a half.", "start": 1384.15, "duration": 4.2}, {"text": "Half of it minus one of it\nof a transpose S inverse a.", "start": 1388.35, "duration": 7.5}, {"text": "Sorry, that's not brilliant\nuse of the blackboard to squeeze that in there.", "start": 1395.85, "duration": 5.52}, {"text": "But that's easily repeatable.", "start": 1401.37, "duration": 5.01}, {"text": "OK, good.", "start": 1406.38, "duration": 3.39}, {"text": "So that's what a quadratic bowl,\na perfect quadratic problem minimizes to that's\nits lowest level.", "start": 1409.77, "duration": 10.620000000000001}, {"text": "Ooh, I wanted to mention\none other function, because I'm going to speak\nmostly about quadratics, but obviously,\nthe whole point is that it's the convexity that's\nreally making things work.", "start": 1420.39, "duration": 16.13}, {"text": "So here, let me just put here,\na remarkable convex function.", "start": 1436.52, "duration": 10.67}, {"text": "And the notes tell what's the\ngradient of this function.", "start": 1451.8, "duration": 8.89}, {"text": "They don't actually go\nas far as the Hessian.", "start": 1460.69, "duration": 3.86}, {"text": "Proving that this function I'm\ngoing to write down is convex, it takes a little thinking.", "start": 1464.55, "duration": 10.17}, {"text": "But it's a fantastic function.", "start": 1474.72, "duration": 3.09}, {"text": "You would never\nsort of imagine it if you didn't see it sometime.", "start": 1477.81, "duration": 6.300000000000001}, {"text": "So it's going to be a function\nof a matrix, a function of-- those are n squared\nvariables, x, i, j.", "start": 1484.11, "duration": 14.52}, {"text": "So it's a function\nof many variables.", "start": 1498.63, "duration": 2.51}, {"text": "And here is this function.", "start": 1501.14, "duration": 2.08}, {"text": "It's you take the\ndeterminant of the matrix.", "start": 1503.22, "duration": 4.08}, {"text": "That's clearly a function of\nall the n squared variables.", "start": 1507.3, "duration": 3.71}, {"text": "Then you take the log\nof the determinant and put in a minus sign\nbecause we want convex.", "start": 1511.01, "duration": 10.83}, {"text": "That turns out to be\na convex function.", "start": 1521.84, "duration": 2.82}, {"text": "And even to just check that\nfor 2 by 2 well, for 2 by 2 you have four variables,\nbecause it's a 2 by 2 matrix.", "start": 1524.66, "duration": 7.529999999999999}, {"text": "We could maybe check it\nfor a symmetric matrix.", "start": 1532.19, "duration": 2.97}, {"text": "I move it down to\nthree variables.", "start": 1535.16, "duration": 2.01}, {"text": "But I'd be glad anybody\nwho's ambitious to see why that log determinant\nis a remarkable function.", "start": 1537.17, "duration": 14.28}, {"text": "And let me see.", "start": 1551.45, "duration": 1.2}, {"text": "So the gradient of that\nthing is also amazing.", "start": 1556.04, "duration": 5.82}, {"text": "The gradient of that function-- I'm going to peek so I don't\nwrite the wrong fact here.", "start": 1561.86, "duration": 9.75}, {"text": "So the partial derivative\nof that function are the entries of-- these are the entries\nof a, a inverse.", "start": 1575.78, "duration": 10.44}, {"text": "That's the-- of x inverse.", "start": 1586.22, "duration": 1.74}, {"text": "That's like, wow.", "start": 1598.36, "duration": 1.52}, {"text": "Where did that come from?", "start": 1599.88, "duration": 2.25}, {"text": "It might be minus the\nentries, of course.", "start": 1602.13, "duration": 3.28}, {"text": "Yeah, yeah, yeah.", "start": 1605.41, "duration": 1.52}, {"text": "So we've got n\nsquared function-- what is a typical\nentry in x inverse?", "start": 1606.93, "duration": 9.629999999999999}, {"text": "What does a typical\nx inverse i, j?", "start": 1616.56, "duration": 5.53}, {"text": "Just to remember\nthat bit of pretty old fashioned linear\nalgebra, the entry is of the inverse matrix,\nI'm sure to divide by what?", "start": 1622.09, "duration": 12.89}, {"text": "The determinant, that's\nthe one thing we know.", "start": 1634.98, "duration": 2.22}, {"text": "And that's the reason\nwe take the log, because when you take\nderivatives of a log, that will put determinant\nof x in the denominator.", "start": 1641.72, "duration": 9.959999999999999}, {"text": "And then the numerator\nwill be the derivatives of the determinant of x.", "start": 1651.68, "duration": 4.48}, {"text": "Oh!", "start": 1656.16, "duration": 0.5}, {"text": "Can we get any idea what are the\nderivatives of the determinant?", "start": 1656.66, "duration": 4.98}, {"text": "Oh my god.", "start": 1661.64, "duration": 1.956}, {"text": "How did I never get into this?", "start": 1663.596, "duration": 2.814}, {"text": "So are you with me so far?", "start": 1666.41, "duration": 3.68}, {"text": "This is going to be\nderivatives of determinant, the strength of all\nthese variables divided by the determinant, because\nthat's what the log achieved.", "start": 1670.09, "duration": 12.04}, {"text": "So when I take the derivative\nof the log of something, that chain rule says take the\nderivative of that something divide by the function\ndeterminant of x.", "start": 1682.13, "duration": 13.77}, {"text": "So what's the derivative of\nthe determinant of a matrix with respect to its 1, 1 entry?", "start": 1695.9, "duration": 6.609999999999999}, {"text": "Yeah, sure.", "start": 1702.51, "duration": 0.5}, {"text": "This is crazy.", "start": 1703.01, "duration": 1.95}, {"text": "But it's crazy to be doing this.", "start": 1704.96, "duration": 1.53}, {"text": "But it's healthy.", "start": 1706.49, "duration": 1.51}, {"text": "OK.", "start": 1708.0, "duration": 0.5}, {"text": "So I have a matrix x, da,\nda, da, x, x, 1, 1, x, 1n, et cetera, xn, 1, x, n, n.", "start": 1711.96, "duration": 11.44}, {"text": "OK.", "start": 1723.4, "duration": 1.65}, {"text": "And what am I looking for?", "start": 1725.05, "duration": 1.39}, {"text": "I'm looking for that for\nthe derivatives of the-- do I want the derivatives\nof the determinant?", "start": 1726.44, "duration": 9.19}, {"text": "Yes.", "start": 1735.63, "duration": 1.92}, {"text": "So what's the derivative of x\nof the determinant with respect to the first equals what?", "start": 1737.55, "duration": 12.55}, {"text": "How can I figure out?", "start": 1753.78, "duration": 2.17}, {"text": "So what's this asking me to do?", "start": 1755.95, "duration": 1.86}, {"text": "It's asking me to change x,\n1, 1 by delta x and see what's the change in the determinant.", "start": 1757.81, "duration": 8.17}, {"text": "That's what derivatives are.", "start": 1765.98, "duration": 2.24}, {"text": "Change x, 1, 1 a little bit.", "start": 1768.22, "duration": 2.79}, {"text": "How much did the\ndeterminant change?", "start": 1771.01, "duration": 1.605}, {"text": "What has the determinant\nof the whole matrix got to do with x, 1, 1?", "start": 1776.15, "duration": 6.7}, {"text": "You remember that there is\na formula for determinants.", "start": 1782.85, "duration": 4.42}, {"text": "So I need that fact.", "start": 1787.27, "duration": 1.89}, {"text": "The determinant of x is\nx, 1, 1 times something.", "start": 1789.16, "duration": 6.44}, {"text": "Is that something that\nI really want to know?", "start": 1795.6, "duration": 2.91}, {"text": "Plus x, 1, 2 times\nother something plus say, along the first row\ntimes another something.", "start": 1798.51, "duration": 7.837999999999999}, {"text": "What are these\nfactors that multiply the x's to give the determinant?", "start": 1809.34, "duration": 10.45}, {"text": "What [INAUDIBLE] a\nlinear combination of the first row time certain\nfactors gives the determinant?", "start": 1819.79, "duration": 7.550000000000001}, {"text": "And how do I know that\nthere will be such factors, because the fundamental\nproperty of the determinant is that it's linear in row 1 if\nI don't mess with other rows.", "start": 1827.34, "duration": 11.940000000000001}, {"text": "It's a linear function of row 1.", "start": 1839.28, "duration": 3.96}, {"text": "So it has a form x,\n1, 1 times something.", "start": 1843.24, "duration": 3.27}, {"text": "And what is something?", "start": 1846.51, "duration": 1.774}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 1848.284, "duration": 0.917}, {"text": "GILBERT STRANG: The\ndeterminant of this.", "start": 1849.201, "duration": 3.099}, {"text": "So what does x, 1, 1 multiply\nwhen you compute determinants?", "start": 1852.3, "duration": 4.26}, {"text": "X, 1, 1 will not multiply\nany other guys in its row, because you're never\nmultiplying two x's in the same row\nor the same column.", "start": 1856.56, "duration": 9.72}, {"text": "What x, 1, 1 is\nmultiplying all these guys.", "start": 1866.28, "duration": 3.93}, {"text": "And in fact, it turns out\nto be is the determinant.", "start": 1870.21, "duration": 4.83}, {"text": "And what is this called?", "start": 1875.04, "duration": 2.14}, {"text": "That one smaller determinant\nthat I get by throwing away the first row and first column?", "start": 1877.18, "duration": 7.79}, {"text": "It's called a-- Minor is good.", "start": 1884.97, "duration": 3.91}, {"text": "Yes, minor is good.", "start": 1888.88, "duration": 1.98}, {"text": "I was saying there are two\nwords that can be used, minor and co-factor.", "start": 1890.86, "duration": 6.03}, {"text": "Yeah.", "start": 1902.86, "duration": 0.7}, {"text": "And what is it?", "start": 1903.56, "duration": 1.18}, {"text": "I mean, how do I compute it?", "start": 1904.74, "duration": 1.31}, {"text": "What is the number?", "start": 1906.05, "duration": 1.317}, {"text": "This is a number.", "start": 1907.367, "duration": 0.708}, {"text": "It's just a number.", "start": 1911.18, "duration": 0.93}, {"text": "Maybe I think of the minor\nas this determinant-- Ah!", "start": 1916.88, "duration": 4.87}, {"text": "Let me cancel that.", "start": 1921.75, "duration": 1.73}, {"text": "Maybe I think of the\nminor as this smaller matrix, and the\nco-factor, which is the determinant of the minor.", "start": 1923.48, "duration": 6.945}, {"text": "And there is a plus or minus.", "start": 1935.18, "duration": 1.71}, {"text": "Everything about\ndeterminants, there's a there's a plus or\nminus choice to be made.", "start": 1936.89, "duration": 6.54}, {"text": "And we're not going\nto worry about that.", "start": 1943.43, "duration": 4.17}, {"text": "But so anyway, so\nit's the co-factor.", "start": 1947.6, "duration": 5.725}, {"text": "Let me call it C, 1, 1.", "start": 1953.325, "duration": 1.975}, {"text": "And so that's the formula\nfor a determinant.", "start": 1957.95, "duration": 4.74}, {"text": "That's the co-factor\nexpansion of a determinant.", "start": 1962.69, "duration": 4.152}, {"text": "OK.", "start": 1974.23, "duration": 1.87}, {"text": "And that will connect\nback to this amazing fact that the gradient is the\nentries of x inverse, because the inverse is the ratio\nof co-factor to determinant.", "start": 1976.1, "duration": 11.62}, {"text": "So x inverse 1, 1 is that\nco-factor over the determinant.", "start": 1987.72, "duration": 8.052}, {"text": "Yeah.", "start": 1998.67, "duration": 1.52}, {"text": "So that's where\nthis all comes from.", "start": 2000.19, "duration": 2.34}, {"text": "Anyway, I'm just mentioning that\nas a very interesting example of a convex function.", "start": 2002.53, "duration": 13.290000000000001}, {"text": "OK.", "start": 2015.82, "duration": 1.45}, {"text": "I'll leave that.", "start": 2017.27, "duration": 0.68}, {"text": "That's just for like, education.", "start": 2017.95, "duration": 3.79}, {"text": "OK.", "start": 2021.74, "duration": 1.34}, {"text": "Now I'm ready to go to\nwork on gradient descent.", "start": 2023.08, "duration": 5.43}, {"text": "So actually, the rest of\nthis class and Friday's class about gradient descent are very\nfundamental parts of 18.065.", "start": 2028.51, "duration": 10.8}, {"text": "And that will be\none of our examples.", "start": 2039.31, "duration": 2.44}, {"text": "And then the general case here.", "start": 2041.75, "duration": 4.9}, {"text": "So I'm using this.", "start": 2046.65, "duration": 4.39}, {"text": "It would be interesting\nto minimize that thing, but we're not going there.", "start": 2051.04, "duration": 4.369}, {"text": "Let's hide it, so we\ndon't see it again.", "start": 2055.409, "duration": 5.071}, {"text": "And I'll work with that example.", "start": 2060.48, "duration": 2.55}, {"text": "So here's gradient descent.", "start": 2066.429, "duration": 2.181}, {"text": "Is xk plus 1 is xk\nminus Sk the step size times the gradient of f at xk.", "start": 2077.77, "duration": 9.99}, {"text": "So the only thing\nleft that requires us to input some decision making\nis a step size, the learning rate.", "start": 2092.922, "duration": 10.177999999999999}, {"text": "We can take it as constant.", "start": 2103.1, "duration": 3.42}, {"text": "If we take too big\na learning rate, the thing will oscillate\nall over the place and it's a disaster.", "start": 2106.52, "duration": 9.61}, {"text": "If we take too small a\nlearning rate, too small steps, what's the matter with that?", "start": 2116.13, "duration": 6.470000000000001}, {"text": "Takes too long.", "start": 2122.6, "duration": 1.59}, {"text": "Takes too long.", "start": 2124.19, "duration": 2.07}, {"text": "So the problem is to\nget it just right.", "start": 2126.26, "duration": 4.14}, {"text": "And one way that you\ncould say get it right would be to think of optimize.", "start": 2130.4, "duration": 6.63}, {"text": "Choose the optimal Sk.", "start": 2137.03, "duration": 1.89}, {"text": "Of course, that takes longer\nthan just deciding an Sk in advance, which\nis what people do.", "start": 2138.92, "duration": 7.45}, {"text": "So I'll tell you what people\ndo is on really big problems is take an Sk-- estimate a suitable Sk, and\nthen go with it for a while.", "start": 2146.37, "duration": 11.149999999999999}, {"text": "And then look back to\nsee if it was too big, they'll see oscillations.", "start": 2157.52, "duration": 7.789999999999999}, {"text": "It'll be bouncing\nall over the place.", "start": 2165.31, "duration": 3.91}, {"text": "Or of course, an\nexact line search-- so you see that this\nexpression often.", "start": 2169.22, "duration": 6.664999999999999}, {"text": "The exact line search choose\nSk to make my function f at xk plus 1 a minimum on\nthe line, on the search line, a minimum in the\nsearch direction.", "start": 2179.09, "duration": 29.145}, {"text": "The search direction is\ngiven by the gradient.", "start": 2214.175, "duration": 3.765}, {"text": "That's the direction\nwe're moving.", "start": 2217.94, "duration": 1.83}, {"text": "This is the distance\nwe're moving, or measure of the\ndistance we're moving.", "start": 2219.77, "duration": 5.67}, {"text": "And an exact search would\nbe to go along there.", "start": 2225.44, "duration": 4.14}, {"text": "If I have a convex function,\nthen as I move along this line, as I increase Sk, I'll see\nthe function start down, because the gradient,\nnegative gradient means down.", "start": 2229.58, "duration": 15.8}, {"text": "But at some point\nit'll turn up again.", "start": 2245.38, "duration": 2.7}, {"text": "And an exact line search would\nfind that point and stop there.", "start": 2248.08, "duration": 5.14}, {"text": "That doesn't mean we would-- we will see in\nthis example where we will do exact line searches\nthat for a small value of b, it's extremely slow, that\nthe condition number controls the speed.", "start": 2256.31, "duration": 16.35}, {"text": "That's really what\nmy message will be just in these last\nminutes and next time the sort of key lecture\non gradient descent.", "start": 2272.66, "duration": 10.68}, {"text": "So an exact line\nsearch would be that.", "start": 2283.34, "duration": 3.33}, {"text": "So what a backtracking\nline search-- backtracking would be\ntake a fixed S like one.", "start": 2286.67, "duration": 11.19}, {"text": "And then be prepared\nto come backwards.", "start": 2304.67, "duration": 7.62}, {"text": "Cut back by half.", "start": 2312.29, "duration": 1.77}, {"text": "See what you get at that point.", "start": 2314.06, "duration": 2.19}, {"text": "Cut back by half of that to a\nquarter of the original step.", "start": 2316.25, "duration": 3.93}, {"text": "See what that is.", "start": 2320.18, "duration": 1.02}, {"text": "So the full step might\nhave taken you back to the upward sweep.", "start": 2324.65, "duration": 7.800000000000001}, {"text": "Halfway forward it might\nstill be on the upward sweep.", "start": 2332.45, "duration": 2.97}, {"text": "Might be too much, but so\nbacktracking cuts the step size in pieces and checks until it-- So S0, half of\nS0, quarter of S0, or obviously, a different\nparameter, aS0, a squared S0, and so on until you're\nsatisfied with that step.", "start": 2335.42, "duration": 26.7}, {"text": "And there are of course,\nmany, many refinements.", "start": 2365.72, "duration": 2.35}, {"text": "We're talking about\nthe big algorithm here that everybody has,\ndepending on their function, has different experiences with.", "start": 2368.07, "duration": 16.18}, {"text": "So here's my\nfundamental question.", "start": 2384.25, "duration": 2.42}, {"text": "Let's think of an\nexact line search.", "start": 2390.58, "duration": 3.03}, {"text": "How much does that\nreduce the function?", "start": 2393.61, "duration": 4.09}, {"text": "How much does that\nreduce the function?", "start": 2397.7, "duration": 2.7}, {"text": "So that's really what the\nbounds that I want are.", "start": 2400.4, "duration": 4.98}, {"text": "How much does that\nreduce the function?", "start": 2405.38, "duration": 3.06}, {"text": "And we'll see that the reduction\ninvolves the condition number, m over M. So why don't I\nturn to the example first?", "start": 2408.44, "duration": 24.29}, {"text": "And then where we\nknow exact answers.", "start": 2432.73, "duration": 4.53}, {"text": "That gives us a\nbasis for comparison.", "start": 2437.26, "duration": 2.72}, {"text": "And then our math\ngoal is prove-- get S dead bounds\non the size of f that match what we see\nexactly in that example where we know everything.", "start": 2439.98, "duration": 18.14}, {"text": "We know the gradient.", "start": 2458.12, "duration": 3.39}, {"text": "We know the Hessian.", "start": 2461.51, "duration": 1.63}, {"text": "It's that matrix.", "start": 2463.14, "duration": 0.95}, {"text": "We know the condition number.", "start": 2464.09, "duration": 1.56}, {"text": "So what happens if\nI start at a point x0 y0 that's on my surface?", "start": 2465.65, "duration": 9.455}, {"text": "Sorry.", "start": 2479.11, "duration": 1.12}, {"text": "What do I want to do here?", "start": 2480.23, "duration": 2.48}, {"text": "Yeah.", "start": 2482.71, "duration": 0.54}, {"text": "I take a point, x0\ny0 and I iterate.", "start": 2483.25, "duration": 7.83}, {"text": "So the new xy k plus\n1 is xyk minus the S, which I can compute\ntimes the gradient of f.", "start": 2494.35, "duration": 22.59}, {"text": "So I'm going to\nput in gradient f.", "start": 2516.94, "duration": 1.77}, {"text": "What is the gradient here?", "start": 2518.71, "duration": 1.32}, {"text": "The derivative is\nwe expect to x.", "start": 2522.79, "duration": 3.0}, {"text": "So I have a 2xk and 2by.", "start": 2525.79, "duration": 6.18}, {"text": "And this is the step size.", "start": 2536.63, "duration": 1.614}, {"text": "And for this small\nproblem where we're going to get such\na revealing answer, I'm going to choose\nexact line search.", "start": 2542.12, "duration": 7.74}, {"text": "I'm going to choose the best xk.", "start": 2549.86, "duration": 1.38}, {"text": "And what's the answer?", "start": 2554.04, "duration": 1.2}, {"text": "So I just want to tell you\nwhat the iterations are for that particular\nfunction starting at x0 y0.", "start": 2555.24, "duration": 8.28}, {"text": "So let me put start x0 y0.", "start": 2566.08, "duration": 5.38}, {"text": "And I haven't done this\ncalculation myself.", "start": 2574.81, "duration": 1.98}, {"text": "It's taken from the book by\nSteven Boyd and Vandenberghe called Convex Optimization.", "start": 2576.79, "duration": 6.449999999999999}, {"text": "Of course, they weren't the\nfirst to do this either.", "start": 2583.24, "duration": 2.77}, {"text": "But I'm happy to mention that\nbook Convex Optimization.", "start": 2586.01, "duration": 5.57}, {"text": "And Steven Boyd will be\non campus this spring actually, in April\nfor three lectures.", "start": 2591.58, "duration": 6.6}, {"text": "This is April, maybe.", "start": 2598.18, "duration": 1.83}, {"text": "Yeah, OK.", "start": 2600.01, "duration": 1.0}, {"text": "So it's this month in\ntwo or three weeks.", "start": 2601.01, "duration": 3.39}, {"text": "And I'll tell you about that.", "start": 2604.4, "duration": 2.07}, {"text": "So here are the xk's and the\nyk's and the f and the function values.", "start": 2606.47, "duration": 8.85}, {"text": "So where am I going to start?", "start": 2620.19, "duration": 1.21}, {"text": "Yeah.", "start": 2624.84, "duration": 0.6}, {"text": "So I'm starting from the\npoint x0 y0 equal b1.", "start": 2625.44, "duration": 5.04}, {"text": "Turns out that will make our\nformulas very convenient, x0 y0 equals b1.", "start": 2630.48, "duration": 7.02}, {"text": "Good.", "start": 2637.5, "duration": 0.84}, {"text": "So OK.", "start": 2638.34, "duration": 2.19}, {"text": "So xk is b times the key\nratio b minus 1 over b plus 1 to the kth power.", "start": 2640.53, "duration": 10.89}, {"text": "And yk happens to be-- it has this same ratio.", "start": 2651.42, "duration": 7.665}, {"text": "And my function f has\nthe same ratio too.", "start": 2664.02, "duration": 5.58}, {"text": "This is fk.", "start": 2669.6, "duration": 1.215}, {"text": "It has that same\nratio 1 minus b over 1 plus b to the kth times f0.", "start": 2670.815, "duration": 8.895}, {"text": "That's the beautiful\nformula that we're going to take as the\nbest example possible.", "start": 2679.71, "duration": 14.739999999999998}, {"text": "Let's just see.", "start": 2694.45, "duration": 0.71}, {"text": "If k equals 0, I have xk equal\nb yk equal 1 b starting at b1.", "start": 2695.16, "duration": 9.64}, {"text": "And that tells me the rate\nof decrease of the function.", "start": 2704.8, "duration": 4.89}, {"text": "It's this same ratio.", "start": 2709.69, "duration": 1.99}, {"text": "So what am I learning\nfrom this example?", "start": 2711.68, "duration": 3.05}, {"text": "What's jumping out is that this\nratio 1 minus b over 1 plus b is crucial.", "start": 2714.73, "duration": 6.135}, {"text": "If b is near 1,\nthat ratio is small.", "start": 2725.92, "duration": 3.58}, {"text": "If b is near 1,\nthat's near 0 over 2.", "start": 2729.5, "duration": 3.37}, {"text": "And I converge quickly,\nno problem at all.", "start": 2732.87, "duration": 3.2}, {"text": "But if b is near 0, if my\ncondition number is bad-- so the bad case, the\nhard case is small b.", "start": 2736.07, "duration": 15.36}, {"text": "Of course, when b is small,\nthat ratio is very near 1.", "start": 2755.2, "duration": 6.1}, {"text": "It's below 1.", "start": 2761.3, "duration": 1.29}, {"text": "The ratio is below 1, so\nI'm getting convergence.", "start": 2762.59, "duration": 3.63}, {"text": "I do get convergence.", "start": 2766.22, "duration": 1.14}, {"text": "I do go downhill.", "start": 2767.36, "duration": 2.1}, {"text": "But what happens is I don't\ngo downhill very far until I'm headed back uphill again.", "start": 2769.46, "duration": 6.449999999999999}, {"text": "So the picture to\ndraw for this-- let me change that picture\nto a picture in the xy plane of the level sets.", "start": 2775.91, "duration": 13.49}, {"text": "So the picture really to\nsee is in the xy plane.", "start": 2789.4, "duration": 4.47}, {"text": "The level sets f equal constant.", "start": 2793.87, "duration": 3.525}, {"text": "That's what a level set is.", "start": 2797.395, "duration": 1.545}, {"text": "It's a set of points, x and\ny where f has the same value.", "start": 2798.94, "duration": 4.63}, {"text": "And what do those look like?", "start": 2803.57, "duration": 2.94}, {"text": "Oh, let's see.", "start": 2806.51, "duration": 1.49}, {"text": "I think-- what do you think?", "start": 2810.92, "duration": 2.76}, {"text": "What do the level sets look like\nfor this particular function?", "start": 2813.68, "duration": 6.18}, {"text": "If I look at the curve x\nsquared plus b y squared equal a constant, that's\nwhat the level set is.", "start": 2819.86, "duration": 7.380000000000001}, {"text": "This is x squared plus by\nsquared equal a constant.", "start": 2827.24, "duration": 6.38}, {"text": "What kind of a curve is that?", "start": 2833.62, "duration": 2.782}, {"text": "AUDIENCE: [INAUDIBLE].", "start": 2836.402, "duration": 0.928}, {"text": "GILBERT STRANG:\nThat's an ellipse.", "start": 2837.33, "duration": 2.14}, {"text": "And what's up with that ellipse?", "start": 2839.47, "duration": 2.43}, {"text": "What's the shape of it?", "start": 2841.9, "duration": 2.85}, {"text": "Because there is no\nxy term, that ellipse is like, well lined\nup with the axes.", "start": 2844.75, "duration": 8.43}, {"text": "The major axes of the ellipse\nare in the x and y directions, because there is\nno cross term here.", "start": 2853.18, "duration": 8.969999999999999}, {"text": "We could always have\ndiagonalized our matrix if it wasn't diagonal.", "start": 2862.15, "duration": 5.473}, {"text": "And that wouldn't\nhave changed anything.", "start": 2867.623, "duration": 1.667}, {"text": "So it's just\nrotating this space.", "start": 2869.29, "duration": 3.45}, {"text": "And we've done that.", "start": 2872.74, "duration": 1.35}, {"text": "What do the levels\nset look like?", "start": 2877.57, "duration": 1.56}, {"text": "They're ellipses.", "start": 2879.13, "duration": 1.74}, {"text": "And suppose b is a small number,\nthen what's with the ellipses?", "start": 2880.87, "duration": 5.82}, {"text": "If b is small, I\nhave to go pretty-- I have to take a pretty\nlarge y to match a-- change an x.", "start": 2886.69, "duration": 8.4}, {"text": "I think maybe they're\nellipses of that sort.", "start": 2895.09, "duration": 3.25}, {"text": "Are they?", "start": 2898.34, "duration": 0.5}, {"text": "They're lined up for the axes.", "start": 2904.22, "duration": 2.56}, {"text": "And I hope I'm drawing\nin the right direction.", "start": 2906.78, "duration": 3.83}, {"text": "They're long and thin.", "start": 2910.61, "duration": 3.197}, {"text": "Is that right?", "start": 2913.807, "duration": 0.583}, {"text": "Because I would have\nto take a pretty big y to make up for a small b.", "start": 2914.39, "duration": 5.73}, {"text": "OK.", "start": 2920.12, "duration": 1.71}, {"text": "So what happens\nwhen I'm descending?", "start": 2921.83, "duration": 2.31}, {"text": "This is a narrow valley then.", "start": 2924.14, "duration": 1.77}, {"text": "Think of it as a valley\nwhich comes down steeply in the y direction,\nbut in the x direction I'm crossing the valley slow-- Oh, is that right?", "start": 2925.91, "duration": 14.34}, {"text": "So what happens if I\ntake a point there?", "start": 2940.25, "duration": 4.05}, {"text": "Oh yeah, I remember what to do.", "start": 2944.3, "duration": 2.39}, {"text": "So let's start at that\npoint on that ellipse.", "start": 2946.69, "duration": 4.16}, {"text": "And those were the levels\nsets f equal constant.", "start": 2954.07, "duration": 3.42}, {"text": "So what's the first\nsearch direction?", "start": 2957.49, "duration": 3.49}, {"text": "What direction do\nI move from x0 y0?", "start": 2960.98, "duration": 2.34}, {"text": "Do I move along the ellipse?", "start": 2968.51, "duration": 2.7}, {"text": "Absolutely not, because along\nthe ellipse f is constant.", "start": 2971.21, "duration": 4.28}, {"text": "The gradient direction is\nperpendicular to the ellipse.", "start": 2975.49, "duration": 3.94}, {"text": "So I move perpendicular\nto the ellipse.", "start": 2979.43, "duration": 2.85}, {"text": "And when do I stop?", "start": 2982.28, "duration": 1.005}, {"text": "Pretty soon, because very\nsoon I'm going back up again.", "start": 2987.04, "duration": 3.89}, {"text": "I haven't practiced\nwith this curve.", "start": 3002.41, "duration": 1.71}, {"text": "But I know-- and time\nis up, thank God.", "start": 3004.12, "duration": 4.28}, {"text": "So what do I know\nis going to happen?", "start": 3008.4, "duration": 2.38}, {"text": "And by Friday we'll\nmake it happen?", "start": 3010.78, "duration": 3.0}, {"text": "So what do we see for the\ncurve, the track of the-- it's say it?", "start": 3013.78, "duration": 10.996}, {"text": "AUDIENCE: Zigzag.", "start": 3024.776, "duration": 0.994}, {"text": "GILBERT STRANG:\nIt's a zigzag, yeah.", "start": 3025.77, "duration": 2.34}, {"text": "We would like to get here, but\nwe're not aimed here at all.", "start": 3028.11, "duration": 3.0}, {"text": "So we zig, zig, zig zag,\nand very slowly approach that point.", "start": 3031.11, "duration": 5.43}, {"text": "And how slowly?", "start": 3039.21, "duration": 2.7}, {"text": "With that multiplier, 1\nminus b over 1 plus b.", "start": 3041.91, "duration": 7.08}, {"text": "That's what I'm learning\nfrom this example, that that's a key number.", "start": 3048.99, "duration": 4.02}, {"text": "And then you could ask, well,\nwhat about general examples?", "start": 3053.01, "duration": 3.75}, {"text": "This was one specially chose\nan example with exact solution.", "start": 3056.76, "duration": 4.71}, {"text": "Well, we'll see at the\nbeginning of next time that for a convex\nfunction this is typical.", "start": 3061.47, "duration": 6.93}, {"text": "This is 1 minus b is the\ncritical quantity, or 1 over b, or the how small\nis b compared to 1?", "start": 3068.4, "duration": 9.36}, {"text": "So that will be the\ncritical quantity.", "start": 3077.76, "duration": 2.35}, {"text": "And we see it in this ratio\n1 minus b over 1 plus b.", "start": 3080.11, "duration": 4.28}, {"text": "So if b is 100, this\nis 0.99 over 1.01.", "start": 3084.39, "duration": 5.82}, {"text": "It's virtually 1.", "start": 3090.21, "duration": 1.62}, {"text": "OK.", "start": 3091.83, "duration": 0.63}, {"text": "So next time is a\nsort of a key lecture to see what I've just\nsaid, that this controls the convergence of\nsteepest descent, and then to see an\nidea that speeds it up.", "start": 3092.46, "duration": 18.67}, {"text": "That idea is called\nmomentum or heavy ball.", "start": 3111.13, "duration": 3.53}, {"text": "So the physical idea is if you\nhad a heavy ball right there and wanted to get it down\nthe valley toward the bottom, you wouldn't go perpendicular\nto the level sets.", "start": 3114.66, "duration": 15.99}, {"text": "Not at all.", "start": 3130.65, "duration": 0.63}, {"text": "You'd let the momentum\nof the ball take over and let it roll down.", "start": 3131.28, "duration": 5.71}, {"text": "So the idea of momentum is\nto model the possibility of letting that heavy ball\nroll instead of directing it by the steepest\ndescent at every point.", "start": 3136.99, "duration": 13.39}, {"text": "So there's an extra term in\nsteepest descent, the momentum term that accelerates.", "start": 3150.38, "duration": 5.85}, {"text": "OK.", "start": 3156.23, "duration": 0.63}, {"text": "So Friday is the day.", "start": 3156.86, "duration": 2.67}, {"text": "Good.", "start": 3159.53, "duration": 0.66}, {"text": "See you then.", "start": 3160.19, "duration": 1.94}]